{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f773653c",
   "metadata": {},
   "source": [
    "# Putting Transformers into Production with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafa1e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.5.0.dev0\n",
      "Uninstalling transformers-4.5.0.dev0:\n",
      "  Successfully uninstalled transformers-4.5.0.dev0\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-7w9lb23_\n",
      "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-7w9lb23_\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (0.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (4.59.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (2021.3.17)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.5.0.dev0-py3-none-any.whl size=2059073 sha256=e1b66c84ddc3fe9a0848e3d02e3c8002660b6add8c175b686c6807c18df364fa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b9uh74ey/wheels/05/0a/97/64ae47c27ba95fae2cb5838e7b4b7247a34d4a8ba5f7092de2\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.5.0.dev0\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (1.0.4)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
      "Requirement already satisfied: IProgress in /usr/local/lib/python3.8/dist-packages (0.4)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from IProgress) (1.15.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.6.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.5.0.dev0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install sentence-transformers termcolor IProgress nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b73b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Execution Providers: \n",
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "!echo \"ONNX Runtime Execution Providers: \" && python -c \"import onnxruntime as ort; print(ort.get_available_providers())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd074b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export ORT_TENSORRT_MAX_BATCH_SIZE=10\n",
    "!export ORT_TENSORRT_MAX_WORKSPACE_SIZE=4294967296\n",
    "!export ORT_TENSORRT_MAX_PARTITION_ITERATIONS=20\n",
    "!export ORT_TENSORRT_MIN_SUBGRAPH_SIZE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8de5d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "1.7.1\n",
      "4.5.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "import multiprocessing\n",
    "import transformers\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from termcolor import colored\n",
    "from transformers import convert_graph_to_onnx\n",
    "from pathlib import Path\n",
    "from onnxruntime_customops import get_library_path\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "print(onnx.__version__)\n",
    "print(rt.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29012fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = rt.SessionOptions()\n",
    "opt.register_custom_ops_library(get_library_path())\n",
    "opt.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "opt.log_severity_level = 4\n",
    "opt.intra_op_num_threads = multiprocessing.cpu_count()\n",
    "opt.execution_mode = rt.ExecutionMode.ORT_SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1165d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mGPU available True\u001b[0m\n",
      "\u001b[32mGPU Name: Tesla V100-SXM2-32GB\u001b[0m\n",
      "\u001b[32mGPU Count: 1\u001b[0m\n",
      "\u001b[32mCORE Count: 48\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"GPU available {torch.cuda.is_available()}\", \"green\"))\n",
    "print(colored(f\"GPU Name: {torch.cuda.get_device_name(0)}\", \"green\"))\n",
    "print(colored(f\"GPU Count: {torch.cuda.device_count()}\", \"green\"))\n",
    "print(colored(f\"CORE Count: {multiprocessing.cpu_count()}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0357236",
   "metadata": {},
   "source": [
    "## Simple Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13eee59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "pipeline_name = \"feature-extraction\"\n",
    "model_pth = Path(f\"encoder/{model_name}.onnx\")\n",
    "\n",
    "nlp = transformers.pipeline(pipeline_name, model=model_name, tokenizer=model_name, device=0)\n",
    "model = nlp.model\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "if not model_pth.exists():\n",
    "    convert_graph_to_onnx.convert(\n",
    "        framework=\"pt\",\n",
    "        model=model_name,\n",
    "        output=model_pth,\n",
    "        opset=12,\n",
    "        tokenizer=model_name,\n",
    "        use_external_format= False,\n",
    "        pipeline_name=pipeline_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ad3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 10_000\n",
    "\n",
    "sents = [\" \".join(sent) for sent in nltk.corpus.brown.sents()][:MAX_SENTENCES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31947b84",
   "metadata": {},
   "source": [
    "## Baseline: Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca0d0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 10000 sentences took 106s at 94 sentences/s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for sent in sents:\n",
    "    _ = nlp(sent)\n",
    "\n",
    "duration = int(time.time() - start)\n",
    "speed = int(MAX_SENTENCES / duration)\n",
    "print(f\"encoding {MAX_SENTENCES} sentences took {duration}s at {speed} sentences/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd4edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be4520",
   "metadata": {},
   "source": [
    "## CUDAExecutionProvider + CPUExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ed2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "sess = rt.InferenceSession(str(model_pth), opt, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4efcb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 10000 sentences took 26s at 384 sentences/s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for sent in sents:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)\n",
    "\n",
    "duration = int(time.time() - start)\n",
    "speed = int(MAX_SENTENCES / duration)\n",
    "print(f\"encoding {MAX_SENTENCES} sentences took {duration}s at {speed} sentences/s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547950a",
   "metadata": {},
   "source": [
    "## CUDAExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca12493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\"CUDAExecutionProvider\"]\n",
    "sess = rt.InferenceSession(str(model_pth), opt, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab4169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 10000 sentences took 27s at 370 sentences/s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for sent in sents:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)\n",
    "\n",
    "duration = int(time.time() - start)\n",
    "speed = int(MAX_SENTENCES / duration)\n",
    "print(f\"encoding {MAX_SENTENCES} sentences took {duration}s at {speed} sentences/s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d797c",
   "metadata": {},
   "source": [
    "## TensorrtExecutionProvider + CUDAExecutionProvider + CPUExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74fe75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "sess = rt.InferenceSession(str(model_pth), opt, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e3ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "for sent in sents[:50]:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc06b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 10000 sentences took 109s at 91 sentences/s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for sent in sents:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)\n",
    "\n",
    "duration = int(time.time() - start)\n",
    "speed = int(MAX_SENTENCES / duration)\n",
    "print(f\"encoding {MAX_SENTENCES} sentences took {duration}s at {speed} sentences/s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f75689",
   "metadata": {},
   "source": [
    "## TensorrtExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9d03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\"TensorrtExecutionProvider\"]\n",
    "sess = rt.InferenceSession(str(model_pth), opt, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fa43c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "for sent in sents[:50]:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eab501e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 10000 sentences took 109s at 91 sentences/s.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for sent in sents:\n",
    "    model_input = tokenizer.encode_plus(sent)\n",
    "    model_input = {name : np.atleast_2d(value) for name, value in model_input.items()}\n",
    "    _ = sess.run(None, model_input)\n",
    "\n",
    "duration = int(time.time() - start)\n",
    "speed = int(MAX_SENTENCES / duration)\n",
    "print(f\"encoding {MAX_SENTENCES} sentences took {duration}s at {speed} sentences/s.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
